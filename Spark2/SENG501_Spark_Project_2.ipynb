{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png) + ![Python Logo](http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png)\n",
    "# **Linear Regression Lab**\n",
    "#### This lab covers a common supervised learning pipeline, using a subset of the Deerfoot Trail data introduced in a previous lab. Our goal is to train a linear regression model to predict Deerfoot commute times given weather and accident conditions\n",
    "#### ** This lab will cover: **\n",
    "   #### *Part 1:* Read and parse the initial dataset\n",
    "   #### *Part 2:* Create and evaluate a baseline model\n",
    "   #### *Part 3:* Train and evaluate a linear regression model\n",
    "   #### *Part 5:* Add higher order behaviour and interactions between features\n",
    " \n",
    "#### Note that, for reference, you can look up the details of the relevant Spark methods in [Spark's Python API](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD) and the relevant NumPy methods in the [NumPy Reference](http://docs.scipy.org/doc/numpy/reference/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "done\n"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 1: Read and parse the initial dataset **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (1a) Load and check the data **\n",
    "#### The raw data is currently stored in 3 text files.  Commute time data is stored in one of these files. This is the same file you used in a previous lab.  Weather data for the duration of the commute time dataset is stored in 2 separate files - one for 2013 and one for 2014.  This data was obtained from Environment Canada.  We will start by storing the commute time data in a RDD and the weather data in 2 separate RDDs.  Each element of these RDDs is a comma separated string.  Use the [count method](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.count) to check how many data points/lines we have in each of the RDDs.  Then use the [take method](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.take) to create and print out a list of the first 2 data points in their initial string format from the commute time RDD and first 15 lines from each of the weather RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "read files\n"
    }
   ],
   "source": [
    "fileName1 = 'deerfoot_part2-1.csv'\n",
    "fileName2 = 'eng-daily-01012013-12312013.csv'\n",
    "fileName3 = 'eng-daily-01012014-12312014.csv'\n",
    "deerfootRDD = sc.textFile(fileName1, 8)\n",
    "weather2013RDD = sc.textFile(fileName2,8)\n",
    "weather2014RDD = sc.textFile(fileName3,8)\n",
    "print('read files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "202\n['21/09/2013,Saturday,34,34,34,34,35,34,35,36,38,36,36,35,35,35,35,35,36,34,34,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,2', '22/09/2013,Sunday,34,34,34,34,34,34,34,35,35,35,34,35,34,35,34,34,34,34,34,0,0,0,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3']\n391\n391\n['\"Station Name\",\"CALGARY INTL A\"', '\"Province\",\"ALBERTA\"', '\"Latitude\",\"51.12\"', '\"Longitude\",\"-114.01\"', '\"Elevation\",\"1099.10\"', '\"Climate Identifier\",\"3031092\"', '\"WMO Identifier\",\"71877\"', '\"TC Identifier\",\"YYC\"', '', '\"Legend\"', '\"A\",\"Accumulated\"', '\"C\",\"Precipitation occurred, amount uncertain\"', '\"E\",\"Estimated\"', '\"F\",\"Accumulated and estimated\"', '\"L\",\"Precipitation may or may not have occurred\"']\n['\"Station Name\",\"CALGARY INTL A\"', '\"Province\",\"ALBERTA\"', '\"Latitude\",\"51.12\"', '\"Longitude\",\"-114.01\"', '\"Elevation\",\"1099.10\"', '\"Climate Identifier\",\"3031092\"', '\"WMO Identifier\",\"71877\"', '\"TC Identifier\",\"YYC\"', '', '\"Legend\"', '\"A\",\"Accumulated\"', '\"C\",\"Precipitation occurred, amount uncertain\"', '\"E\",\"Estimated\"', '\"F\",\"Accumulated and estimated\"', '\"L\",\"Precipitation may or may not have occurred\"']\n"
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "numPoints = deerfootRDD.count()\n",
    "print(numPoints)\n",
    "samplePoints = deerfootRDD.take(2)\n",
    "print(samplePoints)\n",
    "num2013WeatherLines = weather2013RDD.count()\n",
    "num2014WeatherLines = weather2014RDD.count()\n",
    "print(num2013WeatherLines)\n",
    "print(num2014WeatherLines)\n",
    "sample2013WeatherLines = weather2013RDD.take(15)\n",
    "sample2014WeatherLines = weather2014RDD.take(15)\n",
    "print(sample2013WeatherLines)\n",
    "print(sample2014WeatherLines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (1b) Preprocessing RDDs - extracting desired fields from commute time data **\n",
    "#### We need to pre-process the RDDs before we can use them for training a regression model.  For this exercise, we will consider predicting the commute time at 5 PM in the evening when traffic is typically heavy.  As a result we won't be using all the fields in the 'deerfootRDD'.  Specifically, we will only be using the following fields - Date (field0), Day(field1), Commute Time at 5 PM (field15), and Total number of accidents (field46).  Use a Spark transformation to create a new pairRDD whose key is the Date and whose value is the tuple (Day, Commute Time at 5 PM, Total number of accidents). You need to use the function 'extractFields' to achieve this.  Print the first 2 elements of the resulting pairRDD as a check to see if you extracted the correct fields., `u'split,me'.split(',')` returns `[u'split', u'me']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[('21/09/2013', ('Saturday', '35', '2'))]\n"
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def extractFields(deerfootRDDRecord):\n",
    "    \"\"\"Creates a key-value pair consisting of the Date field as the key and the the tuple (Day,Commute Time at 5 PM, Total number of Accidents) as the value\n",
    "\n",
    "    Args:\n",
    "        deerfootRDDRecord : a comma separated string consisting of all fields in the data set.\n",
    "\n",
    "    Returns:\n",
    "        extracted record: key-value pair as detailed above\n",
    "    \"\"\"\n",
    "    fieldsList = deerfootRDDRecord.split(',')\n",
    "    kvp = (fieldsList[0], (fieldsList[1], fieldsList[15], fieldsList[46]))\n",
    "    return kvp\n",
    "deerfootPairRDD = deerfootRDD.map(extractFields)\n",
    "print(deerfootPairRDD.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (1c) Preprocessing RDDs - extracting desired lines from weather data **\n",
    "#### If you notice the output of the weather files the first few lines don't contain the data - they contain metadata and column labels.  These need to be removed.  Use a Spark transformation to discard unwanted metadata and header information.  Hint: You can see whether a unicode string string1 contains another string in variable string2 using 'u'string1' in string2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['\"2013-01-01\",\"2013\",\"01\",\"01\",\"�\",\"1.2\",\"\",\"-8.9\",\"\",\"-3.9\",\"\",\"21.9\",\"\",\"0.0\",\"\",\"0.0\",\"T\",\"0.0\",\"\",\"0.0\",\"T\",\"4\",\"\",\"33\",\"\",\"48\",\"\"', '\"2013-01-02\",\"2013\",\"01\",\"02\",\"�\",\"5.8\",\"\",\"-10.9\",\"\",\"-2.6\",\"\",\"20.6\",\"\",\"0.0\",\"\",\"0.0\",\"\",\"0.0\",\"\",\"0.0\",\"\",\"3\",\"\",\"29\",\"\",\"41\",\"\"', '\"2013-01-03\",\"2013\",\"01\",\"03\",\"�\",\"8.8\",\"\",\"-8.8\",\"\",\"0.0\",\"\",\"18.0\",\"\",\"0.0\",\"\",\"0.0\",\"\",\"0.0\",\"\",\"0.0\",\"\",\"2\",\"\",\"28\",\"\",\"52\",\"\"', '\"2013-01-04\",\"2013\",\"01\",\"04\",\"�\",\"2.9\",\"\",\"-11.2\",\"\",\"-4.2\",\"\",\"22.2\",\"\",\"0.0\",\"\",\"0.0\",\"\",\"0.0\",\"\",\"0.0\",\"\",\"2\",\"\",\"26\",\"\",\"63\",\"\"', '\"2013-01-05\",\"2013\",\"01\",\"05\",\"�\",\"4.8\",\"\",\"-10.0\",\"\",\"-2.6\",\"\",\"20.6\",\"\",\"0.0\",\"\",\"0.0\",\"\",\"0.0\",\"\",\"0.0\",\"\",\"1\",\"\",\"27\",\"\",\"41\",\"\"']\n['\"2014-01-01\",\"2014\",\"01\",\"01\",\"�\",\"-0.6\",\"\",\"-9.7\",\"\",\"-5.2\",\"\",\"23.2\",\"\",\"0.0\",\"\",\"0.0\",\"\",\"1.0\",\"\",\"0.8\",\"\",\"9\",\"\",\"\",\"\",\"<31\",\"\"', '\"2014-01-02\",\"2014\",\"01\",\"02\",\"�\",\"8.5\",\"\",\"-10.0\",\"\",\"-0.8\",\"\",\"18.8\",\"\",\"0.0\",\"\",\"0.0\",\"\",\"0.0\",\"\",\"0.0\",\"\",\"11\",\"\",\"27\",\"\",\"52\",\"\"', '\"2014-01-03\",\"2014\",\"01\",\"03\",\"�\",\"3.0\",\"\",\"-11.0\",\"\",\"-4.0\",\"\",\"22.0\",\"\",\"0.0\",\"\",\"0.0\",\"\",\"7.8\",\"\",\"5.8\",\"\",\"8\",\"\",\"36\",\"\",\"54\",\"\"', '\"2014-01-04\",\"2014\",\"01\",\"04\",\"�\",\"-11.0\",\"\",\"-21.9\",\"\",\"-16.5\",\"\",\"34.5\",\"\",\"0.0\",\"\",\"0.0\",\"\",\"1.0\",\"\",\"0.2\",\"\",\"14\",\"\",\"35\",\"\",\"44\",\"\"', '\"2014-01-05\",\"2014\",\"01\",\"05\",\"�\",\"-14.4\",\"\",\"-25.5\",\"\",\"-20.0\",\"\",\"38.0\",\"\",\"0.0\",\"\",\"0.0\",\"T\",\"0.0\",\"\",\"0.0\",\"T\",\"14\",\"\",\"36\",\"\",\"52\",\"\"']\n"
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def filterLines(weatherRDDRecord):\n",
    "    \"\"\" Skips lines with metadata and label information\n",
    "    Args:\n",
    "        weatherRDDRecord : a line from the weather file.\n",
    "\n",
    "    Returns:\n",
    "        True - if line is not metadata/label; False otherwise\n",
    "    \"\"\"\n",
    "    fieldsList = weatherRDDRecord.split(',')\n",
    "    if len(fieldsList) == 27 and fieldsList[0] != '\"Date/Time\"':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "filteredWeather2013RDD = weather2013RDD.filter(filterLines)\n",
    "filteredWeather2014RDD = weather2014RDD.filter(filterLines)\n",
    "print (filteredWeather2013RDD.take(5))\n",
    "print (filteredWeather2014RDD.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (1d) Preprocessing RDDs - Fixing date format inconsistency**\n",
    "#### If you compare the Date fields of the commute time and weather files you will notice that they are in different formats.  While the date in the commute time file is in the format \"day/month/year\" it is in the format \"year-month-day\".  We need to fix things so that the weather data has the same date format as the commute time data. (It is very typical to deal with such annoyances while developing models from different data sources). Use a Spark transformation that constructs a pairRDD whose key is the date and whose value is a tuple containing the rest of the columns of the weather data.  Note, you need to construct 2 pairRDDs, one for 2013 and another for 2014.  Also, as mentioned previously, you need to fix the date formatting such that it matches the formatting in the commute time data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[('\"01\"/\"01\"/\"2013\"', ['\"2013\"', '\"01\"', '\"01\"', '\"�\"', '\"1.2\"', '\"\"', '\"-8.9\"', '\"\"', '\"-3.9\"', '\"\"', '\"21.9\"', '\"\"', '\"0.0\"', '\"\"', '\"0.0\"', '\"T\"', '\"0.0\"', '\"\"', '\"0.0\"', '\"T\"', '\"4\"', '\"\"', '\"33\"', '\"\"', '\"48\"', '\"\"']), ('\"02\"/\"01\"/\"2013\"', ['\"2013\"', '\"01\"', '\"02\"', '\"�\"', '\"5.8\"', '\"\"', '\"-10.9\"', '\"\"', '\"-2.6\"', '\"\"', '\"20.6\"', '\"\"', '\"0.0\"', '\"\"', '\"0.0\"', '\"\"', '\"0.0\"', '\"\"', '\"0.0\"', '\"\"', '\"3\"', '\"\"', '\"29\"', '\"\"', '\"41\"', '\"\"'])]\n[('\"01\"/\"01\"/\"2014\"', ['\"2014\"', '\"01\"', '\"01\"', '\"�\"', '\"-0.6\"', '\"\"', '\"-9.7\"', '\"\"', '\"-5.2\"', '\"\"', '\"23.2\"', '\"\"', '\"0.0\"', '\"\"', '\"0.0\"', '\"\"', '\"1.0\"', '\"\"', '\"0.8\"', '\"\"', '\"9\"', '\"\"', '\"\"', '\"\"', '\"<31\"', '\"\"']), ('\"02\"/\"01\"/\"2014\"', ['\"2014\"', '\"01\"', '\"02\"', '\"�\"', '\"8.5\"', '\"\"', '\"-10.0\"', '\"\"', '\"-0.8\"', '\"\"', '\"18.8\"', '\"\"', '\"0.0\"', '\"\"', '\"0.0\"', '\"\"', '\"0.0\"', '\"\"', '\"0.0\"', '\"\"', '\"11\"', '\"\"', '\"27\"', '\"\"', '\"52\"', '\"\"'])]\n"
    }
   ],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def fixDate(weatherRDDRecord):\n",
    "    \"\"\"Creates a key-value pair - key is date in format 'day/month/year' value - a comma separated string containing other fields of the record\n",
    "    Args:\n",
    "        weatherRDDRecord : a comma separated string consisting of all fields in the weather data set.\n",
    "\n",
    "    Returns:\n",
    "        extracted record: key-value pair as detailed above\n",
    "    \"\"\"\n",
    "    fieldList = weatherRDDRecord.split(',')\n",
    "    year,month,day = fieldList[1], fieldList[2], fieldList[3]\n",
    "    return ('/'.join([day,month,year]), fieldList[1:])\n",
    "   \n",
    "fixedWeather2013RDD = filteredWeather2013RDD.map(fixDate)\n",
    "fixedWeather2014RDD = filteredWeather2014RDD.map(fixDate)\n",
    "print (fixedWeather2013RDD.take(2))\n",
    "print (fixedWeather2014RDD.take(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (1e) Extracting the desired days from weather data**\n",
    "#### commute times have been recorded for the period September 21, 2013 to April 10, 2014.  We need to consider only the weather data for this period while building the linear regression model.  Use a Spark transformation to append the 'fixedWeather2014' RDD to the 'fixedWeather2013' RDD to create a new RDD.  Then, use another Spark transformation to filter out days before September 21, 2013 and days after April 10, 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def filterDates(weatherRDDRecord):\n",
    "    \"\"\"Skips records before September 21, 2013 and after April 10, 2014\n",
    "    Args:\n",
    "        weatherRDDRecord : key-value record where key is date and value is comma separated string of other weather fields\n",
    "\n",
    "    Returns:\n",
    "        True if key is in the desired period; False otherwise\n",
    "    \"\"\"\n",
    "    <FILL IN>\n",
    "\n",
    "aggregateWeatherRDD = <FILL IN>\n",
    "print aggregateWeatherRDD.count()\n",
    "desiredWeatherRDD = aggregateWeatherRDD.<FILL IN>\n",
    "print desiredWeatherRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (1e) Merging commute time and weather data**\n",
    "#### We have to merge the commute time RDD 'deerfootPairRDD' and the weather RDD 'desiredWeatherRDD'.  Use an appropriate Spark transformation that uses the common key date key in both RDDs.  Print the number of elements and the first 5 elements of the resulting RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "combinedPairRDD = deerfootPairRDD.<FILL IN>\n",
    "print combinedPairRDD.count()\n",
    "print combinedPairRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (1f) Filter out weekends and unwanted weather fields to produce fina data for regression**\n",
    "#### Predicting commute times is more crucial for weekdays than for weekends, which have very light traffic.  So, we will exclude saturdays and sundays from the combined dataset.  We will also only consider using the number of accidents and the \"Snow on the ground\" fields of the combined dataset while building our predictive model.  While other fields, e.g., temperature, might also impact commute times, we will ignore them for the sake of simplicity.  As a result, we need to filter out the unwanted fields.  Use appropriate Spark transformations to produce an RDD whose each record has the format \"commute time, number of accidents, snow on the ground\". If the \"snow on the ground\" field is \"\", i.e., data was not logged that day, then assume that it is \"0\", i.e., assume that day had 0 snow on the ground."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def filterWeekends(combinedPairRDDRecord):\n",
    "    \"\"\"Skips records corresponding to Saturdays and Sundays\n",
    "    Args:\n",
    "        combinedPairRDDRecord : key-value record where key is date and value is a tuple containing comma-separated string values from deerfootPairRDD and desiredWeatherRDD \n",
    "\n",
    "    Returns:\n",
    "        True if key is in the desired period; False otherwise\n",
    "    \"\"\"\n",
    "       <FILL IN>\n",
    "    \n",
    "def extractRegressionData(combinedPairRDDWeekdaysRecord):\n",
    "    \"\"\"Outputs a string record with the format \"commute time,number of accidents, snow on ground\"\n",
    "    Args:\n",
    "        combinedPairRDDWeekdaysRecord: key-value record where key is date and value is a tuple containing comma-separated string values from deerfootPairRDD and desiredWeatherRDD \n",
    "    \n",
    "    Returns:\n",
    "        record with the format \"commute time,number of accidents, snow on ground\n",
    "    \"\"\"\n",
    "       <FILL IN>\n",
    "\n",
    "combinedPairRDDWeekdays = combinedPairRDD.<FILL IN>\n",
    "regressionDataRDD = combinedPairRDDWeekdays.<FILL IN>\n",
    "print regressionDataRDD.count()\n",
    "print regressionDataRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (1g) Using `LabeledPoint` **\n",
    "#### We are now ready to use MLlib to do regression!  In MLlib, labeled training instances are stored using the [LabeledPoint](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LabeledPoint) object.  Write the parsePoint function that takes as input a raw data point in 'regressionDataRDD', parses it using Python's [unicode.split](https://docs.python.org/2/library/string.html#string.split) method, and returns a `LabeledPoint`.  Use this function to parse samplePoints (from the previous question).  Then print out the features and label for the first training point, using the `LabeledPoint.features` and `LabeledPoint.label` attributes. Finally, calculate the number features for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import numpy as np,numpy\n",
    "# Here is a sample raw data point:\n",
    "# u'49,14,16'\n",
    "# In this raw data point, 49 is the label, and the remaining values are features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def parsePoint(line):\n",
    "    \"\"\"Converts a comma separated unicode string into a `LabeledPoint`.\n",
    "\n",
    "    Args:\n",
    "        line (unicode): Comma separated unicode string where the first element is the label and the\n",
    "            remaining elements are features.\n",
    "\n",
    "    Returns:\n",
    "        LabeledPoint: The line is converted into a `LabeledPoint`, which consists of a label and\n",
    "            features.\n",
    "    \"\"\"\n",
    "        <FILL IN>\n",
    "\n",
    "parsedSamplePoints = regressionDataRDD.<FILL IN>\n",
    "firstPoint = parsedSamplePoints.take(1)\n",
    "firstPointFeatures =<FILL IN>\n",
    "firstPointLabel = <FILL IN>\n",
    "print firstPointFeatures, firstPointLabel\n",
    "# d contains the number of features\n",
    "d = <FILL IN>\n",
    "print d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(1h) Feature scaling **\n",
    "#### In learning problems, it is often necessary to \"scale\" features so that all features span nearly the same range.  One way to do this is to do (featureValue-meanOfFeatureValues)/standardDeviationOfFeatureValues.  For a given feature, the mean and standard deviation of its feature values are calculated.  Then, the mean is subtracted from each value and the result is then divided by the standard deviation. \n",
    "\n",
    "#### We will now do feature scaling for our dataset.  First implement a 'getNormalizedRDD' function that takes the RDD with non-scaled values and returns an RDD containing scaled features.  This function will first calculate the mean and standard deviations of the features in the non-scaled RDD.  It will then use a Spark transformation to scale each element of the non-scaled RDD.  This transformation should use the 'normalizeFeatures' closure listed below. Note that your implementation should generalize to any arbitrary number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "\n",
    "def normalizeFeatures(lp):\n",
    "    \"\"\"Normalizes features in the LabeledPoint object lp.\n",
    "\n",
    "    Args:\n",
    "        lp - LabeledPoint object \n",
    "\n",
    "    Returns:\n",
    "        LabeledPoint: The object contains the label and the normalized features\n",
    "    \"\"\"\n",
    "    <FILL IN>\n",
    "    \n",
    "    \n",
    "def getNormalizedRDD(nonNormalizedRDD): \n",
    "    \"\"\"Normalizes the features of the LabeldPoints contained in nonNormalizedRDD.\n",
    "\n",
    "    Args:\n",
    "        nonNormalizedRDD - RDD containing non-normalized features \n",
    "\n",
    "    Returns:\n",
    "        returnRDD: RDD containing normalized features\n",
    "    \"\"\"\n",
    "\n",
    "    <FILL IN>\n",
    "\n",
    "normalizedSamplePoints = <FILL IN>\n",
    "print normalizedSamplePoints.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (1i) Training and validation sets **\n",
    "#### We're almost done parsing our dataset, and our final task involves split it into a training set and a validation set. Use the [randomSplit method](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.randomSplit) with the specified weights and seed to create RDDs storing each of these datasets. Next, cache each of these RDDs, as we will be accessing them multiple times in the remainder of this lab. Finally, compute the size of each dataset and verify that the sum of their sizes equals the value computed in Part (1f)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "weights = [.8, .2]\n",
    "seed = 42\n",
    "parsedTrainData, parsedValData = normalizedSamplePoints.<FILL IN>\n",
    "parsedTrainData.<FILL IN>\n",
    "parsedValData.<FILL IN>\n",
    "nTrain =<FILL IN>\n",
    "nVal = <FILL IN>\n",
    "\n",
    "print nTrain, nVal, nTrain + nVal\n",
    "print normalizedSamplePoints.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 2: Create and evaluate a baseline model **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2a) Average label **\n",
    "#### A very simple yet natural baseline model is one where we always make the same prediction independent of the given data point, using the average label in the training set as the constant prediction value.  Compute this value, which is the average commute time for the training set.  Use an appropriate method in the [RDD API](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "averageCommuteTime = parsedTrainData.<FILL IN>\n",
    "                   \n",
    "print averageCommuteTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2b) Root mean squared error **\n",
    "#### We naturally would like to see how well this naive baseline performs.  We will use root mean squared error ([RMSE](http://en.wikipedia.org/wiki/Root-mean-square_deviation)) for evaluation purposes.  Implement a function to compute RMSE given an RDD of (label, prediction) tuples, and test out this function on an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "import math\n",
    "def squaredError(label, prediction):\n",
    "    \"\"\"Calculates the the squared error for a single prediction.\n",
    "\n",
    "    Args:\n",
    "        label (float): The correct value for this observation.\n",
    "        prediction (float): The predicted value for this observation.\n",
    "\n",
    "    Returns:\n",
    "        float: The difference between the `label` and `prediction` squared.\n",
    "    \"\"\"\n",
    "        <FILL IN>\n",
    "\n",
    "def calcRMSE(labelsAndPreds):\n",
    "    \"\"\"Calculates the root mean squared error for an `RDD` of (label, prediction) tuples.\n",
    "\n",
    "    Args:\n",
    "        labelsAndPred (RDD of (float, float)): An `RDD` consisting of (label, prediction) tuples.\n",
    "\n",
    "    Returns:\n",
    "        float: The square root of the mean of the squared errors.\n",
    "    \"\"\"\n",
    "        <FILL IN>\n",
    "labelsAndPreds = sc.parallelize([(3., 1.), (1., 2.), (2., 2.)])\n",
    "exampleRMSE = calcRMSE(labelsAndPreds)\n",
    "print exampleRMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(2c) Training validation RMSE **\n",
    "#### Now let's calculate the training and validation RMSE of our baseline model. To do this, first create RDDs of (label, prediction) tuples for each dataset, and then call calcRMSE. Note that each RMSE can be interpreted as the average prediction error for the given dataset (in terms of commute time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "labelsAndPredsTrain = parsedTrainData.<FILL IN>\n",
    "rmseTrainBase =<FILL IN>\n",
    "labelsAndPredsVal = parsedValData.<FILL IN>\n",
    "rmseValBase = <FILL IN>\n",
    "print 'Baseline Train RMSE = {0:.3f}'.format(rmseTrainBase)\n",
    "print 'Baseline Validation RMSE = {0:.3f}'.format(rmseValBase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 3: Train linear regression models using MLlib  **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3a) `LinearRegressionWithSGD` **\n",
    "#### MLlib's [LinearRegressionWithSGD](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LinearRegressionWithSGD) implements linear regression with advanced functionality, such as stochastic gradient approximation and allowing both L1 or L2 regularization.  First use LinearRegressionWithSGD to train a model with L2 regularization and with an intercept.  This method returns a [LinearRegressionModel](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LinearRegressionModel).  Next, use the model's [weights](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LinearRegressionModel.weights) and [intercept](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LinearRegressionModel.intercept) attributes to print out the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import DenseVector\n",
    "from pyspark.mllib.regression import LinearRegressionWithSGD\n",
    "# Values to use when training the linear regression model\n",
    "numIters = 500  # iterations\n",
    "alpha = 1.0  # step\n",
    "miniBatchFrac = 1.0  # miniBatchFraction\n",
    "reg = 1e-1  # regParam\n",
    "regType = 'l2'  # regType\n",
    "useIntercept = True  # intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "firstModel = LinearRegressionWithSGD.<FILL IN>\n",
    "# weightsLR1 stores the model weights; interceptLR1 stores the model intercept\n",
    "weightsLR1 =  <FILL IN>\n",
    "interceptLR1 =  <FILL IN>\n",
    "print weightsLR1, interceptLR1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(3b) Predict**\n",
    "#### Now use the [LinearRegressionModel.predict()](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LinearRegressionModel.predict) method to make a prediction on a sample point.  Pass the `features` from a `LabeledPoint` into the `predict()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "samplePoint = parsedTrainData.<FILL IN>\n",
    "samplePrediction = firstModel.<FILL IN>\n",
    "print samplePrediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (3c) Evaluate RMSE **\n",
    "#### Next evaluate the accuracy of this model on the validation set.  Use the `predict()` method to create a `labelsAndPreds` RDD, and then use the `calcRMSE()` function from Part (2b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "labelsAndPreds = parsedValData.<FILL IN>\n",
    "rmseValLR1 = <FILL IN>\n",
    "\n",
    "print rmseValBase\n",
    "print rmseValLR1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (3d) Trying a 2nd order model **\n",
    "#### The linear model is already outperforming the baseline on the validation set by more than a minute on average, but let's see if we can do better. Let's see if a 2nd order model will lead to better predictions. A 2nd order model will have as features [num of accidents, snoOnGround, (num of accidents)^2, (snoOnGround)^2,(num of accidents * snoOnGround)].  First, we will use a Spark transformation to convert the 'parsedSamplePoints' RDD to an RDD that has LabeledPoint objects for the 2nd order modelling. Next, we will scale the features of 2nd order RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "def transformOrderTwo(lp):\n",
    "    \"\"\"Transforms the features in the LabeledPoint object lp into higher order features.\n",
    "\n",
    "    Args:\n",
    "        lp - LabeledPoint object \n",
    "\n",
    "    Returns:\n",
    "        LabeledPoint: The object contains the label and the higher order features\n",
    "    \"\"\"\n",
    "      <FILL IN> \n",
    "\n",
    "orderTwoParsedSamplePoints = parsedSamplePoints.<FILL IN> \n",
    "print orderTwoParsedSamplePoints.take(2)\n",
    "normalizedOrderTwoSamplePoints = <FILL IN> \n",
    "print normalizedOrderTwoSamplePoints.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (3e) Creating training and validation sets for 2nd order model **\n",
    "#### Follow the procedure in 1(i) to create a training and validation set for the 2nd order model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "weights = [.8, .2]\n",
    "seed = 42\n",
    "parsedTrainDataOrderTwo, parsedValDataOrderTwo = normalizedOrderTwoSamplePoints.<FILL IN> \n",
    "\n",
    "nTrain = <FILL IN> \n",
    "nVal = <FILL IN> \n",
    "\n",
    "print nTrain, nVal, nTrain + nVal\n",
    "print normalizedOrderTwoSamplePoints.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (3f) Train the 2nd order model using the training set **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "secondModel = LinearRegressionWithSGD.<FILL IN> \n",
    "# weightsLR1 stores the model weights; interceptLR1 stores the model intercept\n",
    "weightsLR2 = <FILL IN> \n",
    "interceptLR2 =  <FILL IN> \n",
    "print weightsLR2, interceptLR2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (3g) Evaluate RMSE of 2nd order model **\n",
    "#### This is similar to what you did in 3(c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Replace <FILL IN> with appropriate code\n",
    "labelsAndPredsOrderTwo = parsedValDataOrderTwo.<FILL IN> \n",
    "rmseValLR2 = <FILL IN> \n",
    "\n",
    "print rmseValBase\n",
    "print rmseValLR1\n",
    "print rmseValLR2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** Conclusions**\n",
    "#### You would have noticed a slight improvement in accuracy with the 2nd order model from the 1st order model. To get further improvements we could try a few other things such as using an even higher level model, using more features from the commute time and weather data, and using more data.  We could also use more advaned regression techniques supported by MLlib such as ridge regression.  These are left as future exercises that you can try on your own.\n",
    "\n",
    "#### To finish things off, here is a plot that compares the actual commute times to those predicted by our best model.  This is basically a a color-coded scatter plot visualizing tuples storing i) the predicted value from this model and ii) true label.  Lighter colour predictions represent lower errors while the red ones represent large errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "# force Agg\n",
    "matplotlib.use('Agg')\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import ListedColormap, Normalize\n",
    "from matplotlib.cm import get_cmap\n",
    "\n",
    "cmap = get_cmap('YlOrRd')\n",
    "\n",
    "def preparePlot(xticks, yticks, figsize=(10.5, 6), hideLabels=False, gridColor='#999999',\n",
    "                gridWidth=1.0):\n",
    "    \"\"\"Template for generating the plot layout.\"\"\"\n",
    "    plt.close()\n",
    "    fig, ax = plt.subplots(figsize=figsize, facecolor='white', edgecolor='white')\n",
    "    ax.axes.tick_params(labelcolor='#999999', labelsize='10')\n",
    "    for axis, ticks in [(ax.get_xaxis(), xticks), (ax.get_yaxis(), yticks)]:\n",
    "        axis.set_ticks_position('none')\n",
    "        axis.set_ticks(ticks)\n",
    "        axis.label.set_color('#999999')\n",
    "        if hideLabels: axis.set_ticklabels([])\n",
    "    plt.grid(color=gridColor, linewidth=gridWidth, linestyle='-')\n",
    "    map(lambda position: ax.spines[position].set_visible(False), ['bottom', 'top', 'left', 'right'])\n",
    "    return fig, ax\n",
    "\n",
    "predictions = np.asarray(parsedValDataOrderTwo\n",
    "                         .map(lambda lp: secondModel.predict(lp.features))\n",
    "                         .collect())\n",
    "actual = np.asarray(parsedValDataOrderTwo\n",
    "                    .map(lambda lp: lp.label)\n",
    "                    .collect())\n",
    "error = np.asarray(parsedValDataOrderTwo\n",
    "                   .map(lambda lp: (lp.label, secondModel.predict(lp.features)))\n",
    "                   .map(lambda (l, p): squaredError(l, p))\n",
    "                   .collect())\n",
    "\n",
    "norm = Normalize()\n",
    "clrs = cmap(np.asarray(norm(error)))[:,0:3]\n",
    "\n",
    "fig, ax = preparePlot(np.arange(0, 120, 20), np.arange(0, 120, 20))\n",
    "ax.set_xlim(20, 60), ax.set_ylim(20, 100)\n",
    "plt.scatter(predictions, actual, s=14**2, c=clrs, edgecolors='#888888', alpha=0.75, linewidths=.5)\n",
    "ax.set_xlabel('Predicted'), ax.set_ylabel(r'Actual')\n",
    "pass\n",
    "fig.savefig('temp.png')\n",
    "Image(filename='temp.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}